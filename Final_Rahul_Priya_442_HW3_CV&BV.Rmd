---
title: "Rahul_Priya_442_HW3_CV&BV"
author: "Rahul Rajput"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

Question 4 - Cross Validation

Part 1 - Constructing Dateset
```{r}
library(ggplot2)
set.seed(3)
n = 10000
x = runif(n,0,1)
e = rnorm(n,0,0.5)
y = 3*(x^5) + 2*(x^2) + e

data = data.frame(x,y)
#data
```
Part 2 - Splitting dataset into train and test
```{r}
sample = sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.8,0.2))
train = data[sample, ]
test = data[!sample, ]
```

Part 3 - Performing Cross Validation
```{r}
# Splitting the train set into 5 folds
folds = split(train, sample(1:5, nrow(data), replace=T))
fold_vector = c(1,2,3,4,5)
MSE_matrix = matrix(,nrow=10,ncol=5)

for (d in 1:10) {
  for (i in 1:5) {
  test_tmp = data.frame(folds[i])
  colnames(test_tmp) = c('x','y')
  train_tmp = data.frame(folds[i])[FALSE,]
  colnames(train_tmp) = c('x','y')
  
  new_vector = setdiff(fold_vector,i)
  
  for (j in new_vector) {
    to_bind = data.frame(folds[j])
    colnames(to_bind) = c('x','y')
    train_tmp = rbind(train_tmp,to_bind)
  }
  
  train_model = lm(y ~ poly(x,d), data=train_tmp)
  test_predictions = predict(train_model, newdata = test_tmp)
  mse = mean((test_tmp$y - test_predictions)^2)
  MSE_matrix[d,i] = mse
  }
}

MSE_df = data.frame(MSE_matrix)
MSE_df$Average_MSE = rowMeans(MSE_df)
Min_MSE = min(MSE_df$Average_MSE)
sprintf("The lowest MSE was %f.",Min_MSE)
```

Part 4
```{r}

MSE_fulltrain = c() # All the Mean Square Errors
for (d in 1:10) {
  model_fulltrain = lm(y ~ poly(x,d), data=train)
  predictions = predict(model_fulltrain, newdata=test)
  
  MSE = mean((test$y - predictions)^2)
  MSE_fulltrain = append(MSE_fulltrain,MSE)
}

MSE_final = cbind(MSE_df$Average_MSE,data.frame(MSE_fulltrain))
colnames(MSE_final) = c('Train_MSEs','Test_MSEs') # Combining output into a dataframe
#MSE_final
```
```{r}
MSE_final$PolyOrders= 1:10
#MSE_final

ggplot(MSE_final, aes(x=PolyOrders)) +              
  geom_point(aes(y=Train_MSEs), colour="red") +  
  geom_point(aes(y=Test_MSEs), colour="blue") +
  labs(x="Polynomial Orders", y="MSEs")

sprintf("For the first order polynomial model the MSE obtained from using CV on just the training set was lower than the MSE obtained from training on the complete model and testing on the test set. However, for all other higher order polynomial models the Test MSE was consistently lower than the Train MSE. The best performance for both cases was for a polynomial of order 4. The outputs from the CV model and the model trained once on the entire training dataset were extremely similar proving the value of Cross Validation technique.")
```
Question 5 - Bias Variance Tradeoff

```{r}
## Part 1
set.seed(1) #43 #99 #1 #532
BV_matrix = matrix(,nrow=10,ncol=2)
matrix_actual_y = matrix(, nrow = 1, ncol = 1000)
matrix_predictedvalues = matrix(, nrow = 10, ncol = 1000)

for (i in 1:1000) {
  # Create Dataset
  #set.seed(runif(1,0,1000))
  n = 100
  x_bv = runif(n,0,1)
  e_bv = rnorm(n,0,0.5)
  y_bv = 3*(x_bv^5) + 2*(x_bv^2) + e_bv
  df = data.frame(x_bv,y_bv)
  # Storing means of 'true' y
  actual_y = mean(y_bv)
  matrix_actual_y[1,i] = mean(y_bv)  
  
  # Training model for each order and storing predicted values
  for (d in 1:10) {
    model_bv = lm(y_bv ~ poly(x_bv,degree=d), data=df)
    prediction = predict(model_bv, newdata = data.frame(x_bv = c(1.01)))
    matrix_predictedvalues[d,i] = prediction
  }
}

## Part 2
y_actual = 3*(1.01^5) + 2*(1.01^2)

#################################################################
predicted_means_perD = matrix(,nrow=10,ncol=1)
for (i in 1:10) {
  predicted_means_perD[i,1] = mean(matrix_predictedvalues[i,])
}

new = predicted_means_perD - mean(matrix_actual_y[1,])
#################################################################

# Calculating Variance and Bias
for (d in 1:10) {
  variance = var(matrix_predictedvalues[d,])
  BV_matrix[d,1] = variance
  
  for (j in 1:1000) {
    #bias_vector = matrix_actual_y[1,] - matrix_predictedvalues[d,j]
    bias_vector = y_actual - matrix_predictedvalues[d,j]
  }
  
  bias = mean(bias_vector)
  BV_matrix[d,2] = (abs(bias))^2
}

BV_df = data.frame(BV_matrix)
BV_df$PolyOrder = c(1:10)
colnames(BV_df) = c('Variance','Bias','PolyOrder')
BV_df

# Plotting Bias and Variance

sprintf("Bias - Blue ; Variance - Red")
ggplot(BV_df, aes(x=PolyOrder)) +              
  geom_line(aes(y=Variance), colour="red") +  
  geom_line(aes(y=Bias), colour="blue") +
  labs(x="Polynomial Orders", y="Variance and Bias")
```

Part 3A
```{r}
## Part 1
set.seed(1) #43 #99 #1 #532
BV_matrix = matrix(,nrow=10,ncol=2)
matrix_actual_y = matrix(, nrow = 1, ncol = 1000)
matrix_predictedvalues = matrix(, nrow = 10, ncol = 1000)

for (i in 1:1000) {
  # Create Dataset
  #set.seed(runif(1,0,1000))
  n = 100
  x_bv = runif(n,0,10)
  e_bv = rnorm(n,0,0.5)
  y_bv = 3*(x_bv^5) + 2*(x_bv^2) + e_bv
  df = data.frame(x_bv,y_bv)
  # Storing means of 'true' y
  actual_y = mean(y_bv)
  matrix_actual_y[1,i] = mean(y_bv)  
  
  # Training model for each order and storing predicted values
  for (d in 1:10) {
    model_bv = lm(y_bv ~ poly(x_bv,degree=d), data=df)
    prediction = predict(model_bv, newdata = data.frame(x_bv = c(1.01)))
    matrix_predictedvalues[d,i] = prediction
  }
}

## Part 2
y_actual = 3*(1.01^5) + 2*(1.01^2)

# Calculating Variance and Bias
for (d in 1:10) {
  variance = var(matrix_predictedvalues[d,])
  BV_matrix[d,1] = variance
  
  for (j in 1:1000) {
    #bias_vector = matrix_actual_y[1,] - matrix_predictedvalues[d,j]
    bias_vector = y_actual - matrix_predictedvalues[d,j]
  }
  
  bias = mean(bias_vector)
  BV_matrix[d,2] = abs(bias)
}

BV_df = data.frame(BV_matrix)
BV_df$PolyOrder = c(1:10)
colnames(BV_df) = c('Variance','Bias','PolyOrder')
BV_df

# Plotting Bias and Variance

sprintf("Bias - Blue ; Variance - Red")
ggplot(BV_df, aes(x=PolyOrder)) +              
  #geom_line(aes(y=Variance), colour="red") +  
  geom_line(aes(y=Bias), colour="blue") +
  labs(x="Polynomial Orders", y="Variance and Bias")

ggplot(BV_df, aes(x=PolyOrder)) +              
  geom_line(aes(y=Variance), colour="red") +  
  #geom_line(aes(y=Bias), colour="blue") +
  labs(x="Polynomial Orders", y="Variance and Bias")
```
Part 3B
```{r}
## Part 1
set.seed(1) #43 #99 #1 #532
BV_matrix = matrix(,nrow=10,ncol=2)
matrix_actual_y = matrix(, nrow = 1, ncol = 1000)
matrix_predictedvalues = matrix(, nrow = 10, ncol = 1000)

for (i in 1:1000) {
  # Create Dataset
  #set.seed(runif(1,0,1000))
  n = 100
  x_bv = runif(n,0,1)
  e_bv = rnorm(n,0,0.5)
  y_bv = 3*(x_bv^5) + 2*(x_bv^2) + e_bv
  df = data.frame(x_bv,y_bv)
  # Storing means of 'true' y
  actual_y = mean(y_bv)
  matrix_actual_y[1,i] = mean(y_bv)  
  
  # Training model for each order and storing predicted values
  for (d in 1:10) {
    model_bv = lm(y_bv ~ poly(x_bv,degree=d), data=df)
    prediction = predict(model_bv, newdata = data.frame(x_bv = c(-0.5)))
    matrix_predictedvalues[d,i] = prediction
  }
}

## Part 2
y_actual = 3*(1.01^5) + 2*(1.01^2)

# Calculating Variance and Bias
for (d in 1:10) {
  variance = var(matrix_predictedvalues[d,])
  BV_matrix[d,1] = variance
  
  for (j in 1:1000) {
    #bias_vector = matrix_actual_y[1,] - matrix_predictedvalues[d,j]
    bias_vector = y_actual - matrix_predictedvalues[d,j]
  }
  
  bias = mean(bias_vector)
  BV_matrix[d,2] = abs(bias)
}

BV_df = data.frame(BV_matrix)
BV_df$PolyOrder = c(1:10)
colnames(BV_df) = c('Variance','Bias','PolyOrder')
BV_df

# Plotting Bias and Variance

sprintf("Bias - Blue ; Variance - Red")
ggplot(BV_df, aes(x=PolyOrder)) +              
  #geom_line(aes(y=Variance), colour="red") +  
  geom_line(aes(y=Bias), colour="blue") +
  labs(x="Polynomial Orders", y="Variance and Bias")

ggplot(BV_df, aes(x=PolyOrder)) +              
  geom_line(aes(y=Variance), colour="red") +  
  #geom_line(aes(y=Bias), colour="blue") +
  labs(x="Polynomial Orders", y="Variance and Bias")

```

```{r}
sprintf("In the first plot we see that the variance increases in an exponential manner as the the complexity of the model goes up, and the bias decreases exponentially. Interestingly, this is despite the model running on an out of sample value. It could be due to the value being extremely close to the in sample data.")

sprintf("When we sample x from the distribution U[0,10], it would have an outsized effect on the values of y. More particularly the term with x^5 would have a very high effect on the y values and the other terms would have a very small effect, in essence it is lowering the relative magnitude of the random error. This would allow higher order polynomials to predict the values quite well. Consequently we see that both, Bias and Variance, decrease exponenetially as the model complexity increases.")

sprintf("When we try to predict for out of sample data, especially negative, the results are predictably bad and similar for all the models. The bias and variance are quite low for lower orders of polynomials, however for higher order polynomial models the bias and variance increase exponentially. This could be due to extreme overfitting.")
```



